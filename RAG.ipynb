{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175bfb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Install dependencies\n",
    "# -------------------------------\n",
    "# (Uncomment if running in Colab/Jupyter)\n",
    " !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dd9af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Environment Setup\n",
    "# -------------------------------\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- Azure OpenAI Settings ---\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# --- Azure Search Settings ---\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "index_name = os.getenv(\"AZURE_INDEX_NAME\")\n",
    "\n",
    "# --- Azure Storage Settings ---\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "\n",
    "# Check variables\n",
    "for k in [\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\"AZURE_OPENAI_KEY\",\"AZURE_OPENAI_VERSION\",\"AZURE_OPENAI_DEPLOYMENT\",\n",
    "    \"AZURE_SEARCH_ENDPOINT\",\"AZURE_SEARCH_KEY\",\"AZURE_INDEX_NAME\",\n",
    "    \"AZURE_STORAGE_CONNECTION_STRING\",\"AZURE_STORAGE_CONTAINER\"\n",
    "]:\n",
    "    print(f\"{k:40}: {'FOUND' if os.getenv(k) else 'MISSING'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b54901",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Connect to Azure Blob Storage\n",
    "# -------------------------------\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "print(f\"‚úÖ Connected to container: {container_name}\")\n",
    "print(\"Files in container:\", [b.name for b in container_client.list_blobs()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6b0c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Clear existing index\n",
    "# -------------------------------\n",
    "search_client = SearchClient(search_endpoint, index_name, AzureKeyCredential(search_key))\n",
    "\n",
    "def clear_index():\n",
    "    \"\"\"Deletes all docs from the index in batches.\"\"\"\n",
    "    while True:\n",
    "        count = search_client.get_document_count()\n",
    "        print(f\"Current count: {count}\")\n",
    "        if count == 0:\n",
    "            print(\"All docs deleted ‚úÖ\")\n",
    "            break\n",
    "\n",
    "        results = search_client.search(\"*\", select=[\"id\"], top=1000)\n",
    "        ids = [doc[\"id\"] for doc in results]\n",
    "\n",
    "        if not ids:\n",
    "            break\n",
    "\n",
    "        search_client.delete_documents([{\"id\": doc_id} for doc_id in ids])\n",
    "        print(f\"Deleted {len(ids)} docs\")\n",
    "\n",
    "# Run once\n",
    "clear_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51431d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Index documents from Blob Storage\n",
    "# -------------------------------\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=azure_deployment,\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    ")\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=search_endpoint,\n",
    "    azure_search_key=search_key,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    try:\n",
    "        print(f\"üìÇ Processing {blob.name}\")\n",
    "\n",
    "        # Download blob to temp file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(blob.name)[-1]) as tmp_file:\n",
    "            downloader = container_client.download_blob(blob.name)\n",
    "            tmp_file.write(downloader.readall())\n",
    "            tmp_path = tmp_file.name\n",
    "\n",
    "        # Select loader\n",
    "        if blob.name.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(tmp_path)\n",
    "        elif blob.name.lower().endswith((\".docx\", \".doc\")):\n",
    "            loader = UnstructuredWordDocumentLoader(tmp_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping unsupported file: {blob.name}\")\n",
    "            continue\n",
    "\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_file\"] = blob.name\n",
    "\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        vector_store.add_documents(split_docs)\n",
    "\n",
    "        print(f\"‚úÖ Indexed {len(split_docs)} chunks from {blob.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {blob.name}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237ea94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Test query\n",
    "# -------------------------------\n",
    "query = \"Genetically Modified and Novel Foods (Labelling) (England) Regulations 2000\"\n",
    "results = vector_store.similarity_search(query=query, k=3, search_type=\"hybrid\")\n",
    "\n",
    "for i, r in enumerate(results, start=1):\n",
    "    source = r.metadata.get(\"source_file\", \"unknown\")\n",
    "    chunk_id = r.metadata.get(\"chunk_id\") or r.metadata.get(\"id\") or \"unknown\"\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(\"Source file :\", source)\n",
    "    print(\"Chunk id    :\", chunk_id)\n",
    "    print(\"Snippet     :\", r.page_content[:400].replace(\"\\n\", \" \"), \"...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
